# scripts/load_docs.py

import requests
from bs4 import BeautifulSoup
from app.embedding import get_embedding
from app.pinecone_util import upsert_vector
import time
import re
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

BASE_URL = "https://react.dev"
START_PATH = "/learn"

# âœ… ì „ì²´ ë§í¬ ìë™ ìˆ˜ì§‘ í•¨ìˆ˜
def collect_internal_links(start_path: str) -> list[str]:
    visited = set()
    to_visit = [start_path]

    while to_visit:
        path = to_visit.pop()
        if path in visited:
            continue
        visited.add(path)

        full_url = f"{BASE_URL}{path}"
        try:
            res = requests.get(full_url)
            if res.status_code != 200:
                continue
            soup = BeautifulSoup(res.text, "html.parser")
            for link in soup.find_all("a", href=True):
                href = link["href"]
                # /learn/...ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‚´ë¶€ ë¬¸ì„œ ë§í¬ë§Œ
                if href.startswith("/learn") and href not in visited:
                    to_visit.append(href)
        except:
            continue

    return list(visited)

# âœ… ë³¸ë¬¸ ì •ì œ
def clean_text(html):
    soup = BeautifulSoup(html, "html.parser")
    main = soup.find("main") or soup
    for tag in main(["nav", "footer", "header", "script", "style"]):
        tag.decompose()
    return main.get_text(separator="\n").strip()

# âœ… í…ìŠ¤íŠ¸ ë¶„í• 
def split_into_chunks(text, max_chars=800):
    chunks, chunk = [], ""
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        if len(chunk) + len(line) > max_chars:
            chunks.append(chunk.strip())
            chunk = line
        else:
            chunk += " " + line
    if chunk:
        chunks.append(chunk.strip())
    return chunks

# âœ… í¬ë¡¤ë§ ë° Pinecone ì €ì¥ ì „ì²´ ìˆ˜í–‰
def crawl_and_store_all():
    all_paths = collect_internal_links(START_PATH)
    print(f"ğŸ”— ì´ {len(all_paths)}ê°œì˜ í˜ì´ì§€ ìˆ˜ì§‘ë¨")

    for path in all_paths:
        url = f"{BASE_URL}{path}"
        print(f"...í¬ë¡¤ë§ ì¤‘: {url}")
        try:
            response = requests.get(url)
            if response.status_code != 200:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url}")
                continue

            text = clean_text(response.text)
            chunks = split_into_chunks(text)

            for i, chunk in enumerate(chunks):
                embedding = get_embedding(chunk)
                doc_id = f"{path.replace('/', '_')}_{i}"
                upsert_vector(doc_id, embedding, {
                    "text": chunk,
                    "source": url
                })
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {doc_id}")
                time.sleep(0.3)  # OpenAI API ì œí•œ ëŒ€ë¹„
        except Exception as e:
            print(f"â— ì—ëŸ¬ ë°œìƒ ({url}): {e}")

if __name__ == "__main__":
    crawl_and_store_all()
